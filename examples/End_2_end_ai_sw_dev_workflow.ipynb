{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End AI-Assisted Software Development Workflow\n",
    "\n",
    "This paper introduces an end-to-end AI-assisted software development workflow designed to minimize human oversight at each stage of the pipeline while enabling user intervention where necessary. The proposed workflow integrates high-level requirements, detailed design, code generation, and automated unit testing, ensuring that artifacts are well-documented at every stage. By defining a clear pipeline for human-machine interaction with specific goals, this approach addresses the limitations of current AI coding assistants, which often operate only at isolated stages of software development.\n",
    "\n",
    "While the primary focus of this paper is on software development, the same workflow can be applied to a variety of other domains. The workflow leverages AI large language models (LLMs) to transform high-level task descriptions into detailed, pseudo-language low-level descriptions. This intermediate step allows developers to manually review pipeline artifacts and exercise control over the process at the pseudo-language level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "Large Language Models (LLMs) are pattern inference machines trained on vast datasets. While they can generate plausible solutions, their effectiveness is influenced by how clearly requirements are presented, LLM model and complexity of problem. Common challenges include:\n",
    "\n",
    "- **Non-Typical Library Usage**: Users might want to use a library in a unique way, which does not match the pattern the LLM was trained with. LLMs will require specific guidance to accommodate.\n",
    "- **Version Mismatch**: LLMs may use the wrong version of a library or language, which can lead to compatibility issues or broken code.\n",
    "- **Unique Applications**: LLMs are great at guessing, but if your solution is unique, it requires the developer to provide extensive details.\n",
    "- **Version of LLM**: will greatly affect output. Models have improved signifactly in the past year. GPT-4o and GPT-3.5.turbo are used in this paper and the last chapter compairs thier relative performance. Good news is both satisified models satisifed the example requirements but the GPT-4o was far superior. \n",
    "\n",
    "These challenges emphasize the need for well-structured prompts, version control, and diligent review to maximize the benefits of AI-assisted coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting flowdiag.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile flowdiag.txt\n",
    "\n",
    "┌─ #2 ────────┐       ┌─ #5 ────────┐       ┌─ #8 ────────┐       ┌─ #11 ──┐   \n",
    "│ HiLevel Rqmt┌─►#4──►│ LoLevel Rqmt┌─►#7──►│     Code    ┌─►#10─►│UnitTest│   \n",
    "└─────────────┘   ▲   └─────────────┘   ▲   └─────────────┘   ▲   └────────┘   \n",
    "            ┌─ #1 └───────┐       ┌─ #6 └───────┐       ┌─ #9 └───────┐          \n",
    "            │ PseudoC Tmpl│       │ Code Policy │       │ Test Policy │          \n",
    "            └─────────────┘       └─────────────┘       └─────────────┘          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The software development workflow comprises eleven steps, as illustrated in the accompanying diagram. Each section of this paper provides a detailed explanation of a specific step:\n",
    "\n",
    "1. **Pseudocode Template**  \n",
    "2. **High-Level Requirements**  \n",
    "3. **LLM API Interface**  \n",
    "4. **High-Level to Low-Level Requirements Translation**  \n",
    "5. **Low-Level Requirements**  \n",
    "6. **Coding Policy**  \n",
    "7. **Low-Level Requirements to Code Translation**  \n",
    "8. **Code Listing**  \n",
    "9. **Testing Policy**  \n",
    "10. **Code to Unit Test Translation**  \n",
    "11. **Unit Test Listing**  \n",
    "\n",
    "For target applications involving the challenges described above, custom intervention may be required at step 5 to manually edit the *Low-Level Requirements*. While this workflow is demonstrated in a Jupyter Notebook with each step implemented manually, the process can be automated by defining the rules in a Makefile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pseudocode Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting flowdiag.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile flowdiag.txt\n",
    "┌─────────────┐       ┌─────────────┐       ┌─────────────┐       ┌────────┐\n",
    "│ 2.HiLvlRqmt ┼─►#4──►│ 5.LoLvlRqmt ┼─►#7──►│   8.Code    ┼─►#10─►│11.uTest│\n",
    "└─────────────┘   ▲   └─────────────┘   ▲   └─────────────┘   ▲   └────────┘\n",
    "            ╔═ #1 ╚═══════╗       ┌─────┼───────┐       ┌─────┼───────┐          \n",
    "            ║PSEUDO C TMPL║       │ 6.CodePolicy│       │ 9.TestPolicy│          \n",
    "            ╚═════════════╝       └─────────────┘       └─────────────┘               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current AI coding practices often rely on iterative prompting, where developers engage in repetitive back-and-forth interactions to craft the desired output. This approach can be inefficient, leading to frustration and an increased risk of overlooking critical details. An alternative solution involves the use of templates that encapsulate requirements, best practices, and goals within a single, monolithic prompt. The template goal is:\n",
    "\n",
    "- **Higher Efficacy**: Reduces the friction in interactions, minimizing the need for iterative refinement and redundant instructions.\n",
    "- **Leverage Familiar Paradigms**: By framing requirements in popular pseudocode formats, templates align closely with LLM training data, increasing the likelihood of accurate outputs.\n",
    "- **Reusability**: Templates standardize interactions and can be reused across similar projects. This also promotes consistency across the pipeline.\n",
    "- **Human Oversight**: Allows developers to create detailed requirements from high-level instructions and validate AI-generated results to ensure reliability and correctness.\n",
    "\n",
    "LLMs can understand a large variety of pseudocode paradigms, but greater efficiency is gained by leveraging pseudocode paradigms easily understood by LLMs. Pseudocode features that work well with LLMs have:\n",
    "\n",
    "- **Hierarchical Indentation**: Text format defines structure.\n",
    "- **Popularity**: Paradigms that are sufficiently trained into the LLM.\n",
    "- **Expressiveness**: Capable of handling complex logic.\n",
    "\n",
    "The architecture, error handling, and implementation requirements allow fine-tuning of the code and provide workarounds for LLM hallucinations. The template minimizes *redundant or insufficient prompting*, reducing excessive iteration to achieve the desired output and leading to greater efficiency. The YAML template has placeholders for the following parameters used to compile the LLM prompt:\n",
    "\n",
    "- **target_name**: Defines the file name for the code.\n",
    "- **requirements**: Uses a *Step Action Table* and *Structured Function Documentation* that leverage paradigms easily understood by LLMs. The template is extended by adding additional functions and parameters as needed.\n",
    "- **architecture**: Isolates the high-level structure out of the functional requirements section.\n",
    "- **error_handling**: Allows fine-tuning of error handling—could include logging or debug print requirements.\n",
    "- **impl_requirements**: Helpful for refining the implementation or workarounds for hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pseudocode_template.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile pseudocode_template.yaml\n",
    "Format software requirements to conform to the following rules: \n",
    " - Output requirements in a YAML format using the following YAML template.\n",
    " - The template leverages `Step Action Table` and structured documentation standards. \n",
    " - Make sure exception handling is detailed in the `Step Action Table`.\n",
    " - Only list in the `error_handling:` section items not covered in the `Step Action Table`\n",
    " - Ignore any assumptions from previous prompts\n",
    "\n",
    "target_name: <Name of Target>\n",
    "requirements: |\n",
    "  - NAME: <fucntion_name()>\n",
    "    BRIEF: <outline description>\n",
    "    PARAMETERS:\n",
    "      - <param_name>\n",
    "\t    - Type:  <type>\n",
    "\t    - Validate: <define range>\n",
    "\t    - Default: <default value>\n",
    "\t    - Description: <Description> \n",
    "    RETURN: \n",
    "      - <return_name>: <return value description>\n",
    "    CONSTRAINTS: \n",
    "      - <List any constraings>\n",
    "    STEP_ACTION_TABLE: \n",
    "      - STEP_1: |\n",
    "          TITLE: <step_title>\n",
    "          ACTION: <step action>\n",
    "          INPUT: <step input>\n",
    "          OUTPUT: <step output>\n",
    "          NEXT: <next step>\n",
    "\n",
    "architecture: |\n",
    "  - <list all archtecture requirements>\n",
    "\n",
    "error_handling: |\n",
    "  - <list detail for error handling here>\n",
    "\n",
    "impl_requirements: |\n",
    "  - <list implmentation requirements here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. High Level Requirements Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting flowdiag.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile flowdiag.txt\n",
    "╔═════════════╗       ┌─────────────┐       ┌─────────────┐       ┌────────┐\n",
    "║ HI LEVEL REQ║─►#4──►│ 5.LoLvlRqmt ┼─►#7──►│   8.Code    ┼─►#10─►│11.uTest│\n",
    "╚═════════════╝   ▲   └─────────────┘   ▲   └─────────────┘   ▲   └────────┘\n",
    "            ┌─────┼───────┐       ┌─────┼───────┐       ┌─────┼───────┐          \n",
    "            │ 1.pCodeTmpl │       │ 6.CodePolicy│       │ 9.TestPolicy│          \n",
    "            └─────────────┘       └─────────────┘       └─────────────┘          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example shows how to leverage ChatGPT to fill out the pseudocode template. This first manual step converts high-level requirements into a verbose description of the code to be generated.\n",
    "\n",
    "The following is a high-level description for a threadsafe counter. The description is used as a prompt to generate detailed requirements for auto code generation. This high-level description has sufficient features so the resulting detailed requirements need little to no hand editing. There is a diminishing return in iteratively modifying the high-level prompt versus directly editing the detailed requirements prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements_highlevel.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements_highlevel.txt\n",
    " \n",
    "REQUIREMENTS: \n",
    "- Set `target_name:` to `counter.py`\n",
    "- Write requirements for methods that increment, decrement, and retrieve a counter value. \n",
    "- Initialize Counter to zero and value is always non-negative\n",
    "\n",
    "ARCHITECTURE:  Detail in the template `architecture:` section the rules:\n",
    "- Encapsulate functionality into a thread-safe class.\n",
    "- Minimize performance impact under high concurrency.\n",
    "- Handle edge cases and invalid operations predictably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. LLM API\n",
    "\n",
    "We use the OpenAI API to directly translate natural language into low-level requirements, code, or unit tests. Using the API has the following advantages:\n",
    "\n",
    "- **Ignores History**: By ignoring history from previous prompts, the process becomes more repeatable.\n",
    "- **Automation**: Automation can be enabled simply by creating Makefile rules to search the project for requirements files, detect dependencies, generate targets, and run unit tests.\n",
    "- **Temperature**: Controls the creativity of the solution. A value between 0.1 and 1.0 is used, typically set to 0.1 for the most repeatable results.\n",
    "- **OPENAI_API_KEY**: An environment variable that contains your OpenAI API key.\n",
    "- **System Role**: Must be set to define the flavor of the response. For this workflow, we specify a response from an expert in software development. \n",
    "\n",
    "The final chapter discusses the code generation performance differences between GPT models 'gpt-3.5-turbo' and 'gpt-4o' and the following code has an option to use either model so you can make a comparison for yourself. The following class and helper function use the API to translate the prompt into a response string containing low-level requirements, code, or unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import threading\n",
    "import time\n",
    "import re\n",
    "import yaml\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "ENCODING='utf-8'\n",
    "MODEL_KEY_ENV_VARIABLE = \"OPENAI_API_KEY\"\n",
    "\n",
    "class LlmClient:\n",
    "    DEFAULT_TEMPERATURE = 0.1\n",
    "    DEFAULT_MAX_TOKENS = 4000\n",
    "    DEFAULT_MODEL = 'gpt-4o'     # Speed: 17/12/7 seconds\n",
    "    # DEFAULT_MODEL = 'gpt-3.5-turbo'  # Speed: 5/4/4 seconds\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = openai.OpenAI(api_key=os.getenv(MODEL_KEY_ENV_VARIABLE))\n",
    "\n",
    "    def _show_progress(self) -> None:\n",
    "        \"\"\"Print a period every second to indicate progress, with a counter.\"\"\"\n",
    "        self.running = True\n",
    "        seconds = 0\n",
    "        print(\"\\nProgress:\")\n",
    "        while self.running:\n",
    "            seconds += 1\n",
    "            print(f\"\\r{seconds:>3} {'.' * seconds}\", end=\"\", flush=True)\n",
    "            time.sleep(1)\n",
    "\n",
    "    def process_chat(self, messages: List[dict]) -> Tuple[Optional[str], Tuple[int, int, str]]:\n",
    "        \"\"\"Process a chat and return the generated content and token usage.\"\"\"\n",
    "        try:\n",
    "            # Start progress indicator\n",
    "            threading.Thread(target=self._show_progress, daemon=True).start()\n",
    "            response = self.client.chat.completions.create(\n",
    "                messages=messages,\n",
    "                max_tokens=self.DEFAULT_MAX_TOKENS,\n",
    "                temperature=self.DEFAULT_TEMPERATURE,\n",
    "                model=self.DEFAULT_MODEL\n",
    "            )\n",
    "            # Handle and return token usage if available\n",
    "            self.running = False\n",
    "            usage = getattr(response, 'usage', None)\n",
    "            if usage:\n",
    "                return response.choices[0].message.content, (usage.prompt_tokens, usage.completion_tokens, self.DEFAULT_MODEL)\n",
    "            return response.choices[0].message.content, (0, 0, self.DEFAULT_MODEL)\n",
    "        except Exception as e:\n",
    "            self.running = False\n",
    "            print(f\"\\nprocess_chat() An error occurred: {e}\")\n",
    "            return None, (0, 0, self.DEFAULT_MODEL)\n",
    "\n",
    "def process_prompt(prompt: str, system_role: str) -> Tuple[Optional[str], str]:\n",
    "    \"\"\"Example usage of LlmClient with a given prompt.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    client = LlmClient()\n",
    "    result, tokens = client.process_chat(messages)\n",
    "    if result:\n",
    "        token_debug = f\"# TOKENS: {tokens[0] + tokens[1]} (of:{client.DEFAULT_MAX_TOKENS}) = {tokens[0]} + {tokens[1]} (prompt+return) -- MODEL: {tokens[2]}\"\n",
    "        return result, token_debug\n",
    "    return None, \"Failed to generate response.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. High Level Requirements Prompt w/ template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting flowdiag.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile flowdiag.txt\n",
    "┌─────────────┐ ╔═══╗ ┌─────────────┐       ┌─────────────┐       ┌────────┐\n",
    "│ 2.HiLvlRqmt ┼──►#4─►│ 5.LoLvlRqmt ┼─►#7──►│   8.Code    ┼─►#10─►│11.uTest│\n",
    "└─────────────┘ ╚═▲═╝ └─────────────┘   ▲   └─────────────┘   ▲   └────────┘\n",
    "            ┌─────┼───────┐       ┌─────┼───────┐       ┌─────┼───────┐          \n",
    "            │ 1.pCodeTmpl │       │ 6.CodePolicy│       │ 9.TestPolicy│          \n",
    "            └─────────────┘       └─────────────┘       └─────────────┘          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The target requirements are post fixed with the pseudocode_template.yaml to create a prompt for ChatGPT. The call to the OpenAI API translates the prompt into detailed requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress:\n",
      " 13 .............\n",
      " Result Written to: requirements_lowlevel.yaml\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FILE1 = \"requirements_highlevel.txt\"\n",
    "FILE2 = \"pseudocode_template.yaml\"\n",
    "FILE_PROMPT = \"requirements_prompt.txt\"\n",
    "TARGET = \"requirements_lowlevel.yaml\"\n",
    "SYSTEM_ROLE = \"You are and expert in translating high level software requirements into detailed low level requirement sufficent to then be used as a prompt to auto-generate code.\"\n",
    "\n",
    "try:\n",
    "    result = None\n",
    "    e_msg = \"\\nFailed to generate response.\"\n",
    "    prompt = open(FILE1, 'r', encoding='utf-8').read() + '\\n' \n",
    "    prompt +=  open(FILE2, 'r', encoding='utf-8').read()\n",
    "    with open(FILE_PROMPT, 'w', encoding='utf-8') as out:\n",
    "        out.write(SYSTEM_ROLE + '\\n' + prompt)\n",
    "    response, debug = process_prompt(prompt, SYSTEM_ROLE)\n",
    "    if response:\n",
    "        response = re.sub(r'```yaml', '', response)\n",
    "        response = re.sub(r'```.*$', '', response)\n",
    "        result = debug + '\\n' +  response\n",
    "except Exception as e:\n",
    "    e_msg = f\"An error occurred while processing files:\\n  Input files: {FILE1}, {FILE2}\\n  Output file: {FILE_PROMPT}\\n  Error details: {e}\"\n",
    "if result is None:\n",
    "    result = e_msg\n",
    "    print(e_msg)\n",
    "with open(TARGET, 'w', encoding='utf-8') as out:\n",
    "    out.write(result)\n",
    "print(f\"\\n Result Written to: {TARGET}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Detailed Target Requirements Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting flowdiag.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile flowdiag.txt\n",
    "┌─────────────┐       ╔═════════════╗       ┌─────────────┐       ┌────────┐\n",
    "│ 2.HiLvlRqmt ┼─►#4──►║ LO LEVEL REQ║──►#7─►│   8.Code    ┼─►#10─►│11.uTest│\n",
    "└─────────────┘   ▲   ╚═════════════╝   ▲   └─────────────┘   ▲   └────────┘\n",
    "            ┌─────┼───────┐       ┌─────┼───────┐       ┌─────┼───────┐          \n",
    "            │ 1.pCodeTmpl │       │ 6.CodePolicy│       │ 9.TestPolicy│          \n",
    "            └─────────────┘       └─────────────┘       └─────────────┘          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated detailed requirements response will depend on the provider/model LLM used but will always utilize the desired pseudocode template format, as it ensures consistency by framing requirements in a standardized structure that aligns with LLM training data. The following response, 'requirements_prompt.yaml,' was created with ChatGPT 4.0, chosen for its advanced reasoning capabilities and improved consistency compared to earlier models. This method for generating a detailed description has the following advantages:\n",
    "\n",
    "- Generates well-defined, stable code.\n",
    "- Variations are less dependent on the LLM model/version.\n",
    "- Provides verbose material for inline code documentation.\n",
    "- Allows for black-box generation of unit tests.\n",
    "\n",
    "A detailed manual review and editing of the generated YAML requirements are required to ensure fulfillment of application requirements. Common issues to look for include misaligned indentation, incomplete parameter definitions, and incorrect mappings in the Step Action Table. This manual process requires the most human interaction. \n",
    "\n",
    "If the design utilizes a common design pattern, such as Singleton or Factory, or heavily leverages a popular library like NumPy or Flask, then little editing will be needed. A novel algorithm will require detailed editing of the parameters and Step Action Table. The following code displays the auto-generated low-level prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# TOKENS: 1173 (of:4000) = 409 + 764 (prompt+return) -- MODEL: gpt-4o\n",
      "\n",
      "target_name: counter.py\n",
      "requirements: |\n",
      "  - NAME: increment()\n",
      "    BRIEF: Increment the counter value by one.\n",
      "    PARAMETERS: []\n",
      "    RETURN: \n",
      "      - new_value: The updated counter value after increment.\n",
      "    CONSTRAINTS: \n",
      "      - Counter value must remain non-negative.\n",
      "    STEP_ACTION_TABLE: \n",
      "      - STEP_1: |\n",
      "          TITLE: Acquire Lock\n",
      "          ACTION: Acquire a lock to ensure thread safety.\n",
      "          INPUT: None\n",
      "          OUTPUT: Lock acquired\n",
      "          NEXT: STEP_2\n",
      "      - STEP_2: |\n",
      "          TITLE: Increment Counter\n",
      "          ACTION: Increase the counter value by one.\n",
      "          INPUT: Current counter value\n",
      "          OUTPUT: Updated counter value\n",
      "          NEXT: STEP_3\n",
      "      - STEP_3: |\n",
      "          TITLE: Release Lock\n",
      "          ACTION: Release the lock after incrementing.\n",
      "          INPUT: None\n",
      "          OUTPUT: Lock released\n",
      "          NEXT: END\n",
      "\n",
      "  - NAME: decrement()\n",
      "    BRIEF: Decrement the counter value by one, ensuring it does not go below zero.\n",
      "    PARAMETERS: []\n",
      "    RETURN: \n",
      "      - new_value: The updated counter value after decrement.\n",
      "    CONSTRAINTS: \n",
      "      - Counter value must remain non-negative.\n",
      "    STEP_ACTION_TABLE: \n",
      "      - STEP_1: |\n",
      "          TITLE: Acquire Lock\n",
      "          ACTION: Acquire a lock to ensure thread safety.\n",
      "          INPUT: None\n",
      "          OUTPUT: Lock acquired\n",
      "          NEXT: STEP_2\n",
      "      - STEP_2: |\n",
      "          TITLE: Check Counter Value\n",
      "          ACTION: Verify if the counter value is greater than zero.\n",
      "          INPUT: Current counter value\n",
      "          OUTPUT: Boolean indicating if decrement is possible\n",
      "          NEXT: STEP_3\n",
      "      - STEP_3: |\n",
      "          TITLE: Decrement Counter\n",
      "          ACTION: Decrease the counter value by one if possible.\n",
      "          INPUT: Current counter value\n",
      "          OUTPUT: Updated counter value or unchanged if decrement not possible\n",
      "          NEXT: STEP_4\n",
      "      - STEP_4: |\n",
      "          TITLE: Release Lock\n",
      "          ACTION: Release the lock after decrementing.\n",
      "          INPUT: None\n",
      "          OUTPUT: Lock released\n",
      "          NEXT: END\n",
      "\n",
      "  - NAME: get_value()\n",
      "    BRIEF: Retrieve the current counter value.\n",
      "    PARAMETERS: []\n",
      "    RETURN: \n",
      "      - current_value: The current counter value.\n",
      "    CONSTRAINTS: \n",
      "      - Counter value must remain non-negative.\n",
      "    STEP_ACTION_TABLE: \n",
      "      - STEP_1: |\n",
      "          TITLE: Acquire Lock\n",
      "          ACTION: Acquire a lock to ensure thread safety.\n",
      "          INPUT: None\n",
      "          OUTPUT: Lock acquired\n",
      "          NEXT: STEP_2\n",
      "      - STEP_2: |\n",
      "          TITLE: Retrieve Counter Value\n",
      "          ACTION: Return the current counter value.\n",
      "          INPUT: Current counter value\n",
      "          OUTPUT: Current counter value\n",
      "          NEXT: STEP_3\n",
      "      - STEP_3: |\n",
      "          TITLE: Release Lock\n",
      "          ACTION: Release the lock after retrieving the value.\n",
      "          INPUT: None\n",
      "          OUTPUT: Lock released\n",
      "          NEXT: END\n",
      "\n",
      "architecture: |\n",
      "  - Encapsulate functionality into a thread-safe class using locks.\n",
      "  - Minimize performance impact under high concurrency by using efficient locking mechanisms.\n",
      "  - Handle edge cases such as decrementing below zero predictably by preventing such operations.\n",
      "\n",
      "error_handling: |\n",
      "  - Handle lock acquisition failures by retrying or logging an error.\n",
      "  - Ensure that any exceptions during increment or decrement operations do not leave the counter in an inconsistent state.\n",
      "\n",
      "impl_requirements: |\n",
      "  - Use a threading lock to ensure thread safety.\n",
      "  - Initialize the counter to zero and ensure it remains non-negative.\n",
      "  - Optimize lock usage to minimize performance impact under high concurrency.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_name = TARGET\n",
    "try:\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        print(file.read())\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file {file_name} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Coding Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting flowdiag.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile flowdiag.txt\n",
    "┌─────────────┐       ┌─────────────┐       ┌─────────────┐       ┌────────┐\n",
    "│ 2.HiLvlRqmt ┼─►#4──►│ 5.LoLvlRqmt ┼─►#7──►│   8.Code    ┼─►#10─►│11.uTest│\n",
    "└─────────────┘   ▲   └─────────────┘   ▲   └─────────────┘   ▲   └────────┘\n",
    "            ┌─────┼───────┐       ╔═════┼═══════╗       ┌─────┼───────┐          \n",
    "            │ 1.pCodeTmpl │       ║6.CODE POLICY║       │ 9.TestPolicy│          \n",
    "            └─────────────┘       ╚═════════════╝       └─────────────┘          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detailed requirements prompt could contain all the rules for generating the target code, but pulling common rules into a central reusable policy file provides advantages by ensuring these rules are seamlessly integrated into the overall workflow. This integration allows for automated application of consistent policies at each stage of the development cycle, from requirements generation to final code output, reducing redundancy and minimizing manual intervention.\n",
    "\n",
    "- Reduced effort to write requirements.\n",
    "- Consistency across the codebase derived from AI.\n",
    "- Project policies are clearly defined in a central location.\n",
    "- A suite of policies can scale to the size and type of project. A quick prototype would need different policies than a foundational framework.\n",
    "- Policies extract implementation target language rules, allowing the same requirements to translate into various coding languages.\n",
    "\n",
    "The file *rules_python3.8.yaml* defines the common high-level rules for code generation, targeting Python. In the final section of this document, we will demonstrate how these rules can be translated and tested with C++. It has two variables:\n",
    "\n",
    "- *role_system*: Provides the high-level context and sets the flavor of the response.\n",
    "- *role_user*: Contains project high-level rules for code generation. It centralizes coding standards, compiler version, debugging, and error handling, ensuring code consistency. These rules are maintained through periodic reviews and updates, aligned with project milestones or significant changes in project scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting rules_python3.8.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile rules_python3.8.yaml\n",
    "role_system: |\n",
    "  An experienced Python developer skilled in translating software requirements into efficient, maintainable, and Python 3.8-compatible code. \n",
    "  Adheres to best practices such as PEP8, effective use of type hints, and clear documentation. \n",
    "  Implements Pythonic error handling and debugging techniques, ensuring clarity, reliability, and maintainability in all code produced.\n",
    "\n",
    "role_user: |\n",
    "  - Compatibility:\n",
    "    - Generate code compatible with Python version 3.8.\n",
    "\n",
    "  - Coding Standards:\n",
    "    - Follow PEP8 standards.\n",
    "    - Include type hints for all function arguments and return values.\n",
    "    - Import and use generics from the `typing` module (e.g., `List`, `Dict`, `Tuple`) for type hinting.\n",
    "\n",
    "  - Documentation:\n",
    "    - Do not place `Example usage:` at the end of the response. Instead, include inside comment in header.\n",
    "    - Document the file header with best practices, including today's date.\n",
    "    - Include explanations: inside a commont in header documentation.\n",
    "\n",
    "  - Error Handling:\n",
    "    - Use Pythonic error handling practices.\n",
    "    - Include `try-except` blocks and raise appropriate built-in or custom exceptions.\n",
    "    - Provide clear and informative error messages.\n",
    "\n",
    "  - Debugging:\n",
    "    - Use the `logging` module for error and debug information.\n",
    "    - Include a method to print debug statements to stdout, enabled by a `debug_enable` boolean variable (default `False`).\n",
    "    - Provide debug print statements that aid in troubleshooting and understanding code behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Python Translation Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting flowdiag.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile flowdiag.txt\n",
    "┌─────────────┐       ┌─────────────┐ ╔═══╗ ┌─────────────┐       ┌────────┐\n",
    "│ 2.HiLvlRqmt ┼─►#4──►│ 5.LoLvlRqmt ┼──►#7─►│   8.Code    ┼─►#10─►│11.uTest│\n",
    "└─────────────┘   ▲   └─────────────┘ ╚═▲═╝ └─────────────┘   ▲   └────────┘\n",
    "            ┌─────┼───────┐       ┌─────┼───────┐       ┌─────┼───────┐          \n",
    "            │ 1.pCodeTmpl │       │ 6.CodePolicy│       │ 9.TestPolicy│          \n",
    "            └─────────────┘       └─────────────┘       └─────────────┘          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previously defined `LlmClient()` class, which acts as an interface to interact with the OpenAI API, is used by this code snippet to translate the YAML requirement files into target code or unit test software.\n",
    "\n",
    "The following `process()` function is generic and requires a list of YAML keys to extract values from the YAML requirement file. The key/values are used to compose the LLM prompt. There are two key/prefix lists:\n",
    "\n",
    "- **process_req_2_code()**: Requirements → Code translation \n",
    "  - Uses the key/provide list: *key_prefix_pairs_req_2_code*\n",
    "\n",
    "- **process_code_2_test()**: Code → Unit test translation\n",
    "  - Uses the key/provide list: *key_prefix_pairs_code_to_test*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code_from_response(response: str) -> str:\n",
    "    \"\"\"Extract and clean code from a response string.\"\"\"\n",
    "    response = re.sub(r'^.*?\\`\\`\\`', '', response, flags=re.DOTALL)\n",
    "    response = re.sub(r'```.*', '', response, flags=re.DOTALL)\n",
    "    response = re.sub(r'^([\\`]{3})', '#', response, flags=re.MULTILINE)\n",
    "    response = re.sub(r'^python', '#', response, flags=re.MULTILINE)\n",
    "    return response\n",
    "\n",
    "def process(rules_fname: str, req_fname: str, prompt_fname: str, \n",
    "            key_prefix_pairs: list, is_utest: bool) -> None:\n",
    "    \"\"\"\n",
    "    Process the input YAML files to generate a code prompt and save results to specified files.\n",
    "    Parameters:\n",
    "        rules_fname (str): Path to the rules YAML file.\n",
    "        req_fname (str): Path to the requirements YAML file.\n",
    "        prompt_fname (str): File to save the generated code prompt.\n",
    "        is_utest (bool): true if processing unit test translation\n",
    "        key_prefix_pairs (list): List of tuples containing key-prefix pairs for generating prompts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # EXTRACT RULES - a llm prompt must start with high level system role and user role\n",
    "        with open(rules_fname, \"r\", encoding=ENCODING) as file:\n",
    "            data = yaml.safe_load(file)\n",
    "        prompt = [\n",
    "            {\"role\": \"system\", \"content\": data[\"role_system\"]},\n",
    "            {\"role\": \"user\", \"content\": data[\"role_user\"]}\n",
    "        ]\n",
    "\n",
    "        # EXTRACT REQUIREMENTS - from req YAML using `key_prefix_pairs` list \n",
    "        dest_fname = \"unknown.py\"\n",
    "        with open(req_fname, \"r\", encoding=ENCODING) as file:\n",
    "            arch = yaml.safe_load(file)\n",
    "            code_fname = arch.get(\"target_name\", dest_fname)\n",
    "            for key, prefix in key_prefix_pairs:\n",
    "                if key in arch:\n",
    "                    prompt.append({\"role\": \"user\", \"content\": prefix + arch[key]})\n",
    "\n",
    "        # CREATE TARGET FILE NAME - based on the target_name in requirements\n",
    "        if not code_fname.endswith('.py'):\n",
    "            e_msg = f\"The target_name:{code_fname} file name must end with '.py'\"\n",
    "            raise ValueError(e_msg)            \n",
    "        if is_utest:\n",
    "            with open(code_fname, \"r\", encoding=ENCODING) as file:\n",
    "                code = file.read()\n",
    "            prefix = \"Write Unit Test for the following code:\\n\"    \n",
    "            prompt.append({\"role\": \"user\", \"content\": prefix + code})\n",
    "            base_name = code_fname[:-3]  # Remove the .py extension\n",
    "            dest_fname = f\"{base_name}_test.py\"\n",
    "        else:\n",
    "            dest_fname = code_fname\n",
    "\n",
    "        # PROCESS - the chat prompt & save prompt to disk (for inspection)\n",
    "        client = LlmClient()\n",
    "        response, tokens = client.process_chat(prompt)\n",
    "        with open(prompt_fname, 'w', encoding=ENCODING) as out:\n",
    "            out.write(\", \".join(map(str, prompt)).replace(\"\\\\n\", \"\\n\"))\n",
    "\n",
    "        # CONSTRUCT - the result string\n",
    "        result = f\"# TOKENS: {tokens[0] + tokens[1]} (of:{client.DEFAULT_MAX_TOKENS}) = {tokens[0]} + {tokens[1]}(prompt+return) -- MODEL: {tokens[2]}\"\n",
    "        if response is None:\n",
    "            result = \"Failed to generate response.\"\n",
    "        else:\n",
    "            result += extract_code_from_response(response) \n",
    "\n",
    "    except Exception as e:\n",
    "        result = (f\"An error occurred while processing files:\\n  Input files: {rules_fname}, {req_fname}\\n  \"\n",
    "                  f\"Output file: {dest_fname}\\n  Error details: {e}\")\n",
    "        print(f\"ERROR THROWN {result}\")\n",
    "\n",
    "    # Write the result to the destination file\n",
    "    with open(dest_fname, 'w', encoding=ENCODING) as out:\n",
    "        out.write(result)\n",
    "    print(f\"\\n Result Written to: {dest_fname}\\n\")\n",
    "\n",
    "def process_req_2_code(rules_fname: str, req_fname: str, prompt_fname: str) -> None:\n",
    "    is_utest = False\n",
    "    key_prefix_pairs_req_2_code = [\n",
    "        (\"target_name\", \"Code shall be saved in a file named:\"),\n",
    "        (\"requirements\", \"Use the following requirements to write code:\\n\"),\n",
    "        (\"architecture\", \"Use the following architecture to implement code:\\n\"),\n",
    "        (\"interface\", \"Use the following interface implementation requirements:\\n\"),\n",
    "        (\"error_handling\", \"Use the following error handling requirements:\\n\"),\n",
    "        (\"impl_requirements\", \"Use these additional implementation requirements:\\n\")\n",
    "    ]\n",
    "    return process( rules_fname, req_fname, prompt_fname,\n",
    "                   key_prefix_pairs_req_2_code, is_utest )\n",
    "\n",
    "def process_code_2_test(rules_fname: str, req_fname: str, prompt_fname: str) -> None:\n",
    "    is_utest = True\n",
    "    key_prefix_pairs_code_to_test = [\n",
    "        (\"target_name\", \"Code shall be saved in a file named:\"),\n",
    "        (\"test_requirements\", \"see the additional test requirements:\\n\")\n",
    "    ]\n",
    "    return process( rules_fname, req_fname, prompt_fname, \n",
    "                    key_prefix_pairs_code_to_test, is_utest )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Code Translation Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting flowdiag.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile flowdiag.txt\n",
    "┌─────────────┐       ┌─────────────┐       ╔═════════════╗       ┌────────┐\n",
    "│ 2.HiLvlRqmt ┼─►#4──►│ 5.LoLvlRqmt ┼─►#7──►║    8.CODE   ║─►#10─►│11.uTest│\n",
    "└─────────────┘   ▲   └─────────────┘   ▲   ╚═════════════╝   ▲   └────────┘\n",
    "            ┌─────┼───────┐       ┌─────┼───────┐       ┌─────┼───────┐          \n",
    "            │ 1.pCodeTmpl │       │ 6.CodePolicy│       │ 9.TestPolicy│          \n",
    "            └─────────────┘       └─────────────┘       └─────────────┘          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code calls on the LLM to generate the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress:\n",
      " 17 .................\n",
      " Result Written to: counter.py\n",
      "\n",
      "# TOKENS: 1879 (of:4000) = 1094 + 785(prompt+return) -- MODEL: gpt-4o#\n",
      "\"\"\"\n",
      "counter.py\n",
      "\n",
      "This module provides a thread-safe counter class with methods to increment,\n",
      "decrement, and retrieve the current counter value. The counter is initialized\n",
      "to zero and ensures that it remains non-negative. Thread safety is achieved\n",
      "using a threading lock to prevent race conditions during concurrent access.\n",
      "\n",
      "Date: 2023-10-05\n",
      "\n",
      "Features:\n",
      "- Increment the counter value by one.\n",
      "- Decrement the counter value by one, ensuring it does not go below zero.\n",
      "- Retrieve the current counter value.\n",
      "\n",
      "Error Handling:\n",
      "- Handles lock acquisition failures by retrying or logging an error.\n",
      "- Ensures that exceptions during operations do not leave the counter in an inconsistent state.\n",
      "\n",
      "Debugging:\n",
      "- Uses the logging module for error and debug information.\n",
      "- Includes a method to print debug statements to stdout, enabled by a `debug_enable` boolean variable.\n",
      "\"\"\"\n",
      "\n",
      "import threading\n",
      "import logging\n",
      "from typing import Optional\n",
      "\n",
      "# Configure logging\n",
      "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
      "\n",
      "class Counter:\n",
      "    def __init__(self, debug_enable: bool = False) -> None:\n",
      "        self._value = 0\n",
      "        self._lock = threading.Lock()\n",
      "        self.debug_enable = debug_enable\n",
      "\n",
      "    def _log_debug(self, message: str) -> None:\n",
      "        if self.debug_enable:\n",
      "            logging.debug(message)\n",
      "\n",
      "    def increment(self) -> int:\n",
      "        \"\"\"\n",
      "        Increment the counter value by one.\n",
      "\n",
      "        Returns:\n",
      "            new_value (int): The updated counter value after increment.\n",
      "        \"\"\"\n",
      "        self._log_debug(\"Attempting to increment the counter.\")\n",
      "        try:\n",
      "            with self._lock:\n",
      "                self._log_debug(f\"Lock acquired for increment. Current value: {self._value}\")\n",
      "                self._value += 1\n",
      "                self._log_debug(f\"Counter incremented. New value: {self._value}\")\n",
      "        except Exception as e:\n",
      "            logging.error(f\"Failed to increment counter: {e}\")\n",
      "            raise\n",
      "        return self._value\n",
      "\n",
      "    def decrement(self) -> int:\n",
      "        \"\"\"\n",
      "        Decrement the counter value by one, ensuring it does not go below zero.\n",
      "\n",
      "        Returns:\n",
      "            new_value (int): The updated counter value after decrement.\n",
      "        \"\"\"\n",
      "        self._log_debug(\"Attempting to decrement the counter.\")\n",
      "        try:\n",
      "            with self._lock:\n",
      "                self._log_debug(f\"Lock acquired for decrement. Current value: {self._value}\")\n",
      "                if self._value > 0:\n",
      "                    self._value -= 1\n",
      "                    self._log_debug(f\"Counter decremented. New value: {self._value}\")\n",
      "                else:\n",
      "                    self._log_debug(\"Counter value is zero; cannot decrement.\")\n",
      "        except Exception as e:\n",
      "            logging.error(f\"Failed to decrement counter: {e}\")\n",
      "            raise\n",
      "        return self._value\n",
      "\n",
      "    def get_value(self) -> int:\n",
      "        \"\"\"\n",
      "        Retrieve the current counter value.\n",
      "\n",
      "        Returns:\n",
      "            current_value (int): The current counter value.\n",
      "        \"\"\"\n",
      "        self._log_debug(\"Attempting to retrieve the counter value.\")\n",
      "        try:\n",
      "            with self._lock:\n",
      "                self._log_debug(f\"Lock acquired for get_value. Current value: {self._value}\")\n",
      "                current_value = self._value\n",
      "                self._log_debug(f\"Counter value retrieved: {current_value}\")\n",
      "        except Exception as e:\n",
      "            logging.error(f\"Failed to retrieve counter value: {e}\")\n",
      "            raise\n",
      "        return current_value\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rules_fname = \"rules_python3.8.yaml\"\n",
    "req_fname = \"requirements_lowlevel.yaml\"\n",
    "prompt_fname = \"counter_code_prompt.txt\"\n",
    "dest_fname = \"counter.py\"\n",
    "\n",
    "process_req_2_code( rules_fname, req_fname, prompt_fname )\n",
    "with open(dest_fname, \"r\") as file:\n",
    "    content = file.read()\n",
    "print(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Unit Test Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting flowdiag.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile flowdiag.txt\n",
    "┌─────────────┐       ┌─────────────┐       ┌─────────────┐       ┌────────┐\n",
    "│ 2.HiLvlRqmt ┼─►#4──►│ 5.LoLvlRqmt ┼─►#7──►│   8.Code    ┼─►#10─►│11.uTest│\n",
    "└─────────────┘   ▲   └─────────────┘   ▲   └─────────────┘   ▲   └────────┘\n",
    "            ┌─────┼───────┐       ┌─────┼───────┐       ╔═════┼═══════╗          \n",
    "            │ 1.pCodeTmpl │       │ 6.CodePolicy│       ║9.TEST POLICY║          \n",
    "            └─────────────┘       └─────────────┘       ╚═════════════╝          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit test are created in the same process. The following set of rules are applied w/ the code to be tested as a prompt to the LLM to generate the set of Unit Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting rules_ptest.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile rules_ptest.yaml\n",
    "role_system: |\n",
    "  - You are a Python expert specializing in writing high-quality unit tests with the pytest framework.\n",
    "    Generate comprehensive, efficient, and maintainable pytest test cases following best practices.\n",
    "  - Write Python code that adheres to PEP8 standards and is compatible with Python 3.8.\n",
    "  - Always include the import `from typing import Tuple` if type hints are required.\n",
    "\n",
    "role_user: |\n",
    "  - Do not place Explanation: at the end of the response. Instead, include them as header comments in the test case file.\n",
    "  - Ensure test functions cover edge cases and all possible scenarios, including extreme and unexpected inputs.\n",
    "  - Test the function's behavior across a wide range of possible inputs.\n",
    "  - Proactively address edge cases the author might not have considered.\n",
    "  - Ensure tests are deterministic and produce the same result when repeated under the same conditions.\n",
    "\n",
    "  - Use pytest to generate unit tests:\n",
    "    - Employ pytest fixtures appropriately to manage setup and teardown.\n",
    "    - Use pytest parameterization to create concise, readable, and maintainable test cases.\n",
    "    - Do not use pytest's mocking features.\n",
    "\n",
    "  - Organize tests logically to enhance clarity and maintainability.\n",
    "  - Write tests that are easy to read, with clean code and descriptive function names.\n",
    "  - Document the file header with explanations, expectations, and high-level details about the test cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Unit Test Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting flowdiag.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile flowdiag.txt\n",
    "┌─────────────┐       ┌─────────────┐       ┌─────────────┐ ╔═══╗ ┌─────────┐\n",
    "│ 2.HiLvlRqmt ┼─►#4──►│ 5.LoLvlRqmt ┼─►#7──►│   8.Code    ┼─►#10─►│11.uTests│\n",
    "└─────────────┘   ▲   └─────────────┘   ▲   └─────────────┘ ╚═▲═╝ └─────────┘\n",
    "            ┌─────┼───────┐       ┌─────┼───────┐       ┌─────┼───────┐          \n",
    "            │ 1.pCodeTmpl │       │ 6.CodePolicy│       │ 9.TestPolicy│          \n",
    "            └─────────────┘       └─────────────┘       └─────────────┘          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code performs the translation of code into unit test based on the testing policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress:\n",
      "  9 .........\n",
      " Result Written to: counter_test.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rules_fname = \"rules_ptest.yaml\"\n",
    "req_fname = \"requirements_lowlevel.yaml\"\n",
    "prompt_fname = \"counter_unit_test_prompt.txt\"\n",
    "\n",
    "process_code_2_test( rules_fname, req_fname, prompt_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Unit Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting flowdiag.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile flowdiag.txt\n",
    "┌─────────────┐       ┌─────────────┐       ┌─────────────┐       ╔════════╗\n",
    "│ 2.HiLvlRqmt ┼─►#4──►│ 5.LoLvlRqmt ┼─►#7──►│   8.Code    ┼─►#10─►║11.UTEST║\n",
    "└─────────────┘   ▲   └─────────────┘   ▲   └─────────────┘   ▲   ╚════════╝\n",
    "            ┌─────┼───────┐       ┌─────┼───────┐       ┌─────┼───────┐          \n",
    "            │ 1.pCodeTmpl │       │ 6.CodePolicy│       │ 9.TestPolicy│          \n",
    "            └─────────────┘       └─────────────┘       └─────────────┘          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the unit test code listing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# TOKENS: 1591 (of:4000) = 1037 + 554(prompt+return) -- MODEL: gpt-4o#\n",
      "\"\"\"\n",
      "test_counter.py\n",
      "\n",
      "This module contains unit tests for the Counter class defined in counter.py.\n",
      "The tests ensure that the Counter class behaves as expected, covering a range\n",
      "of scenarios including normal operations, edge cases, and concurrent access.\n",
      "\n",
      "Test Cases:\n",
      "- Test initial counter value.\n",
      "- Test incrementing the counter.\n",
      "- Test decrementing the counter.\n",
      "- Test decrementing the counter when the value is zero.\n",
      "- Test concurrent increments and decrements to ensure thread safety.\n",
      "\n",
      "Date: 2023-10-05\n",
      "\"\"\"\n",
      "\n",
      "import pytest\n",
      "from counter import Counter\n",
      "import threading\n",
      "\n",
      "@pytest.fixture\n",
      "def counter():\n",
      "    \"\"\"Fixture to create a new Counter instance for each test.\"\"\"\n",
      "    return Counter()\n",
      "\n",
      "def test_initial_value(counter):\n",
      "    \"\"\"Test that the initial value of the counter is zero.\"\"\"\n",
      "    assert counter.get_value() == 0\n",
      "\n",
      "def test_increment(counter):\n",
      "    \"\"\"Test incrementing the counter.\"\"\"\n",
      "    assert counter.increment() == 1\n",
      "    assert counter.get_value() == 1\n",
      "\n",
      "def test_multiple_increments(counter):\n",
      "    \"\"\"Test multiple increments of the counter.\"\"\"\n",
      "    for _ in range(5):\n",
      "        counter.increment()\n",
      "    assert counter.get_value() == 5\n",
      "\n",
      "def test_decrement(counter):\n",
      "    \"\"\"Test decrementing the counter.\"\"\"\n",
      "    counter.increment()\n",
      "    assert counter.decrement() == 0\n",
      "    assert counter.get_value() == 0\n",
      "\n",
      "def test_decrement_below_zero(counter):\n",
      "    \"\"\"Test that the counter does not go below zero.\"\"\"\n",
      "    assert counter.decrement() == 0\n",
      "    assert counter.get_value() == 0\n",
      "\n",
      "def test_multiple_decrements(counter):\n",
      "    \"\"\"Test multiple decrements of the counter.\"\"\"\n",
      "    for _ in range(3):\n",
      "        counter.increment()\n",
      "    for _ in range(5):\n",
      "        counter.decrement()\n",
      "    assert counter.get_value() == 0\n",
      "\n",
      "def test_concurrent_access():\n",
      "    \"\"\"Test concurrent increments and decrements to ensure thread safety.\"\"\"\n",
      "    counter = Counter()\n",
      "    threads = []\n",
      "\n",
      "    def increment_counter():\n",
      "        for _ in range(1000):\n",
      "            counter.increment()\n",
      "\n",
      "    def decrement_counter():\n",
      "        for _ in range(1000):\n",
      "            counter.decrement()\n",
      "\n",
      "    # Create threads for incrementing and decrementing\n",
      "    for _ in range(10):\n",
      "        t_inc = threading.Thread(target=increment_counter)\n",
      "        t_dec = threading.Thread(target=decrement_counter)\n",
      "        threads.append(t_inc)\n",
      "        threads.append(t_dec)\n",
      "\n",
      "    # Start all threads\n",
      "    for t in threads:\n",
      "        t.start()\n",
      "\n",
      "    # Wait for all threads to complete\n",
      "    for t in threads:\n",
      "        t.join()\n",
      "\n",
      "    # The final value should be zero since increments and decrements are equal\n",
      "    assert counter.get_value() == 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dest_fname = \"counter_test.py\"\n",
    "with open(dest_fname, \"r\") as file:\n",
    "    content = file.read()\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Unit Tests Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting unittest.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile unittest.txt\n",
    "(.venv) preact@oryx:~/sw/ai_sw_workflow$ pytest\n",
    "======== test session starts ===================================\n",
    "platform linux -- Python 3.8.10, pytest-8.3.3, pluggy-1.5.0\n",
    "rootdir: /home/preact/sw/ai_sw_workflow\n",
    "plugins: anyio-4.5.2\n",
    "collected 8 items                                                                                                                         \n",
    "\n",
    "counter_test.py ........                                    [100%]\n",
    "\n",
    "============= 8 passed in 0.40s ==================================\n",
    "("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Performance 'gpt-3.5-turbo' VS 'gpt-4o'\n",
    "\n",
    "Both versions pass thier unit test w/o issue and meet intent of system requirements - but `gpt-4o` code is far suppior. The deficiencies in `3.5-turbo` are evident in the YAML `Low-Level Requirements` file and could be addressed through a combination of adding explicit details to the `High-Level Requirements` or directly editing the `Low-Level Requirements` YAML file. If edits are made to the `Low-Level Requirements`, the automatic translation from high to low-level must be disabled to prevent overwriting your changes.\n",
    "\n",
    "The `3.5-Turbo` model took only 13 seconds to process the pipeline while `4o` took twice as long, 26 seconds. The following table breaks down the pipeline timing:\n",
    "\n",
    "| MODEL           | Total<br> Sec | Req<br> Sec | Code<br> Sec | Test<br> Sec |\n",
    "| --------------- | ------------- | ----------- | ------------ | ------------ |\n",
    "| `GPT-3.5.Turbo` | 13            | 5           | 4            | 4            |\n",
    "| `GPT-4o`        | 26            | 17          | 12           | 7            |\n",
    "\n",
    "LLMs are pattern inference machines. A key takeaway is that 'GPT-4o' successfully infers our intent to use the 'Counting Semaphore' design pattern without requiring explicit instructions. Explicitly calling out the desired design pattern in `High-Level Requirements` significantly improves the `3.5-Turbo` generated code quality.\n",
    "\n",
    "The following tables highlight the differences in artifacts produced by the two LLM models. The most notable differences are:\n",
    "\n",
    "- **Documentation**: `GPT-4o` includes detailed and well-formatted docstrings, which are entirely missing from the Turbo version.\n",
    "- **Thread Safety**: While both versions function correctly, `3.5-Turbo` unnecessarily exposes the counter, violating encapsulation principles. This reduces the robustness of the implementation.\n",
    "\n",
    "\n",
    "**Low Level Requirements Differences**\n",
    "| **Aspect**              | **GPT-3.5-Turbo**                                     | **GPT-4o**                                             |\n",
    "| ----------------------- | ----------------------------------------------------- | ------------------------------------------------------ |\n",
    "| **Thread<br>Safety**       | Described in `architecture`, not implemented in steps | Locking is beautify detailed in Action_Step_Table           |\n",
    "| **Method<br>Parameters**   | Includes parameters for `increment`/`decrement`       | No parameters; operates directly on encapsulated value |\n",
    "| **Edge Case<br>Handling**  | General mentions in `error_handling`                  | Integrated into the method workflows                   |\n",
    "| **Naming**              | Uses `retrieve()`                                     | Uses `get_value()`                                     |\n",
    "| **Concurrency<br>Details** | Minimizes performance impact (general)                | Adds specifics (efficient locking mechanisms)          |\n",
    "\n",
    "**Python Code Generation  Differences**\n",
    "\n",
    "| **Aspect**              | **GPT-3.5-Turbo**                                                                 | **GPT-4o**                                                                                |\n",
    "|-------------------------|-----------------------------------------------------------------------------------|------------------------------------------------------------------------------------------|\n",
    "| **Thread<br>Safety**       | Uses `threading.Lock` but passes a `counter_value` parameter, creating potential redundancy and misuse. | Uses `threading.Lock` consistently, with methods operating directly on the encapsulated counter value. |\n",
    "| **Increment/<br>Decrement** | Unnecessary complexity because `counter_value` is passed in as a parameter, adding complexity and exposing internal state. Exceptions are raised for invalid states. | Excellent code that fixes the poor implementation found in 3.5-Turbo. Handles edge cases (e.g., zero decrement) gracefully without exceptions. |\n",
    "| **Edge Case<br>Handling**  | Raises exceptions for invalid operations like decrementing below zero.             | Treats edge cases predictably (e.g., decrementing at zero is a no-op), avoiding unnecessary exceptions. |\n",
    "| **Code<br>Simplicity**     | Includes unnecessary parameters in methods, complicating the API and violating encapsulation.        | Clean, parameter-free methods ensure consistent and intuitive operation on encapsulated data. |\n",
    "| **Retrieve/<br>Get Value**  | Uses `retrieve()` with thread safety but less standard naming.                     | Uses `get_value()` with thread safety, aligning with Python naming conventions.              |\n",
    "| **Example<br>Usage**       | Demonstrates usage that passes the counter value to methods, contradicting encapsulation. | Demonstrates clean usage directly interacting with the counter object, reinforcing good encapsulation. |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
